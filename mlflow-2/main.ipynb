{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Component 1: MLflow Tracking\n",
    "\n",
    "You can track almost everything using MLflow:  \n",
    "Hyperparameters : n_estimators, max depth, epochs, learning rate, kernel size, dropout, batch size ..  \n",
    "Metrics: AUC, MAE, MSE; F1 score, Accuracy ..  \n",
    "Artifacts:   \n",
    " ❏ models: saved on disk in binary format (pickle, h5, pt, etc.)  \n",
    " ❏ outputs (other than models, e.g. images, csv, text, etc.)  \n",
    "Source: which script / notebook started this experiment.  \n",
    "Tags and Comments: information about the data/features.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"train\") as run:\n",
    "\n",
    "        mlflow.set_tag(\"mlflow.runName\", \"train\")\n",
    "\n",
    "        # create model instance: GBRT (Gradient Boosted Regression Tree)\n",
    "        model = GradientBoostingRegressor(**run_parameters)\n",
    "\n",
    "        # Model Training\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # get evaluations scores\n",
    "        score = rmse_score(y_test, model.predict(X_test))\n",
    "        score_cv = rmse_cv_score(model, X_train, y_train)\n",
    "\n",
    "        # generate charts\n",
    "        # model_feature_importance(model, X_train, model_artifacts_dir)\n",
    "\n",
    "        # log input features\n",
    "        mlflow.set_tag(\"features\", str(X_train.columns.values.tolist()))\n",
    "\n",
    "        # Log tracked parameters\n",
    "        mlflow.log_params(run_parameters)\n",
    "\n",
    "        mlflow.log_metrics(\n",
    "            {\n",
    "                \"RMSE_CV\": score_cv.mean(),\n",
    "                \"RMSE\": score,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # log training loss\n",
    "        for s in model.train_score_:\n",
    "            mlflow.log_metric(\"Train Loss\", s)\n",
    "\n",
    "        # get model signature\n",
    "        signature = infer_signature(\n",
    "            model_input=X_train,\n",
    "            model_output=model.predict(X_train)\n",
    "        )\n",
    "\n",
    "        # Save model to artifacts\n",
    "        mlflow.sklearn.log_model(model, \"model\", signature=signature)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "log_params() — It logs a parameter under the current run.   \n",
    "log_metric() — It logs a metric under the current run.  \n",
    "log_model() — It logs the model as binary file.  \n",
    "infer_signature() — it specifies the model input/output.  \n",
    "set_tags() — It logs input features used to train the model.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Component 2: MLflow Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- code packaging structure that is reusable and repeatable  \n",
    "- This format is documented in a YAML file known as an MLproject file  \n",
    "- The MLproject file must consist of the three basic components as listed below  \n",
    "    Name — A human-readable name given for the project.  \n",
    "    Environment — A software environment that is used to execute project entry points. It includes all library dependencies required by the project code and it supports Conda environment, Docker container environment, as well as system environment.  \n",
    "    Entry Points — It includes commands and information about parameters. Each project must contain at least one entry point, which is called at the beginning of project execution  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3 MLproject file\n",
    "name: mlflow_example\n",
    "\n",
    "conda_env: conda.yaml\n",
    "\n",
    "entry_points:\n",
    "\n",
    "  load_raw_data:\n",
    "    command: \"python preprocess.py\"\n",
    "\n",
    "  train:\n",
    "    command: \"python train.py\"\n",
    "\n",
    "  main:\n",
    "    command: \"python main.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*conda.yaml* is another special file that can be used to declare the conda environment needed to run the steps in this pipeline.  \n",
    "*preprocess.py* will process the raw data into a more training friendly format, performing feature engineering and saving it so that it can be used by next step.  \n",
    "*train.py* will build a model and train it on the data produced by the previous task. Once training finishes the model is put in the artifact store for later use, e.g. serving.  \n",
    "*main.py* is the entry point of the pipeline and will be orchestrating the previous steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main.py file\n",
    "\n",
    "import logging\n",
    "import traceback\n",
    "import warnings\n",
    "\n",
    "import mlflow\n",
    "import click\n",
    "\n",
    "\n",
    "def _run(entrypoint, parameters={}, source_version=None, use_cache=True):\n",
    "    \"\"\"Launching new run for an entrypoint\"\"\"\n",
    "\n",
    "    print(\n",
    "        \"Launching new run for entrypoint=%s and parameters=%s\"\n",
    "        % (entrypoint, parameters)\n",
    "    )\n",
    "    submitted_run = mlflow.run(\".\", entrypoint, parameters=parameters)\n",
    "    return submitted_run\n",
    "\n",
    "\n",
    "@click.command()\n",
    "def workflow():\n",
    "    \"\"\"run the workflow\"\"\"\n",
    "    with mlflow.start_run(run_name=\"data-pipeline\"):\n",
    "        mlflow.set_tag(\"mlflow.runName\", \"data-pipeline\")\n",
    "        _run(\"load_raw_data\")\n",
    "        _run(\"train\", {\"learning_rate\": 0.1, \"max_depth\": 5})\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        filename=\"logs/train_model.log\",\n",
    "        filemode=\"a\",\n",
    "        format=\"%(name)s - %(levelname)s - %(asctime)s - %(message)s\",\n",
    "    )\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    try:\n",
    "        workflow()\n",
    "    except Exception as e:\n",
    "        print(\"Exception occured. Check logs.\")\n",
    "        logger.error(f\"Failed to run workflow due to error:\\n{e}\")\n",
    "        logger.error(traceback.format_exc())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Component 3: MLflow Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the most important step because we can make profit from the model that we built and we make it accessible to users as a real-time serving through a REST API or batch inference on Apache Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dockerized MLflow model serving (REST API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a python script used to fetch the best model trained and to run mlflow model serving in a docker container\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import mlflow\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "PROJECT_DIR = sys.path[0]\n",
    "os.chdir(PROJECT_DIR)\n",
    "\n",
    "experiment_name = 'Default'\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "PORT = 5001  # REST API serving port\n",
    "CONTAINER_NAME = \"mlflow_example_model_serving\"\n",
    "\n",
    "best_run_df = mlflow.search_runs(order_by=['metrics.RMSE_CV ASC'], max_results=1)\n",
    "if len(best_run_df.index) == 0:\n",
    "    raise Exception(f\"Found no runs for experiment '{experiment_name}'\")\n",
    "\n",
    "best_run = mlflow.get_run(best_run_df.at[0, 'run_id'])\n",
    "best_model_uri = f\"{best_run.info.artifact_uri}/model\"\n",
    "# best_model = mlflow.sklearn.load_model(best_model_uri)\n",
    "\n",
    "# print best run info\n",
    "print(\"Best run info:\")\n",
    "print(f\"Run id: {best_run.info.run_id}\")\n",
    "print(f\"Run parameters: {best_run.data.params}\")\n",
    "print(\"Run score: RMSE_CV = {:.4f}\".format(best_run.data.metrics['RMSE_CV']))\n",
    "print(f\"Run model URI: {best_model_uri}\")\n",
    "\n",
    "# remove current container if exists\n",
    "subprocess.run(f\"docker rm --force {CONTAINER_NAME}\", shell=True, check=False, stdout=subprocess.DEVNULL)\n",
    "\n",
    "# run mlflow model serving in a docker container\n",
    "docker_run_cmd = f\"\"\"\n",
    "docker run\n",
    "--name={CONTAINER_NAME}\n",
    "--volume={PROJECT_DIR}:{PROJECT_DIR}\n",
    "--publish {PORT}:{PORT}\n",
    "--interactive\n",
    "--rm\n",
    "mlflow_example\n",
    "mlflow models serve --model-uri {best_model_uri} --host 0.0.0.0 --port {PORT} --workers 2 --no-conda\n",
    "\"\"\".replace('\\n', ' ').strip()\n",
    "print(f\"Running command:\\n{docker_run_cmd}\")\n",
    "\n",
    "subprocess.run(docker_run_cmd, shell=True, check=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have just to run this command: python3 mlflow_model_driver.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0251fbbfe7ed79d0ea37b876af37aad4b5b0621b5b67d3d30962065895503ddb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
